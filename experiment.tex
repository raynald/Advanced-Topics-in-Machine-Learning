\section{Experiments - outperforms state-of-the-art}
\begin{frame}{Experiments}
    \begin{itemize}
        \item 3 datasets (provided by Joachims) 
            \begin{itemize}
                \item Reuters CCAT (800K examples, 47k features)
                \item Physics ArXiv (62k examples, 100k features)
                \item Covertype (581k examples, 54 features)
            \end{itemize}
        \item 4 competing algorithms
            \begin{itemize}
                \item SVM-light (Joachims)
                \item SVM-Perf (Joachims'06)
                \item Norma (Kivinen, Smola, Williamson '02)
                \item Zhang'04 (stochastic gradient descent)
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Training Time(in seconds)}
\begin{table}[h]
    \begin{tabular}{|l|l|l|l|}
            \hline
             & Pegasos & SVM-Perf & SVM-Light \\ \hline
            Reuters & \textbf{2} & 77 & 20,075 \\ \hline
            Covertype & \textbf{6} & 85 & 25,514 \\ \hline
            Astro-Physics & \textbf{2} & 5 & 80 \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Compare to Norma (on Physics)}
\end{frame}

\begin{frame}{Compare to Zhang (on Physics)}
\end{frame}

\begin{frame}{Effect of $k=|A_t|$ when $T$ is fixed}
\end{frame}

\begin{frame}{Effect of $k=|A_t|$ when $kT$ is fixed}
\end{frame}

\section{Extensions}
\begin{frame}{I want my kernels!}
\begin{itemize}
    \item Pegasos can seamlessly be adapted to employ non-linear kernels while working solely on the primal objective function
    \item No need to switch to the dual problem
    \item Number of support vectors is bounded by 
        \[
            \tilde{O}(\frac{1}{\lambda \epsilon})
        \]
\end{itemize}
\end{frame}

\begin{frame}{Complex Decision Problems}
\begin{itemize}
    \item Pegasos works whenever we know how to calculate subgradients of loss func. $l(\wv;(\xv,y))$
    \item Example: Structured output prediction
        \[
            l(\wv;(\xv,y)) = \max_{y'}[\gamma(y,y')-\langle \wv, \phi(\xv,y)- \phi(\xv,y')\rangle]_+
        \]
    \item Subgradient is $\phi(\xv,y')-\phi(\xv,y)$ where $y'$ is the maximizer in the definition of $l$
\end{itemize}
\end{frame}

\begin{frame}{Bias term}
    \begin{itemize}
        \item Popular approach: increase dimension of $x$

            {\color{green}Cons: ``pay'' for $b$ in the regularization term}
        \item Calculate subgradients w.r.t $w$ and w.r.t $b$:

            {\color{green}Cons: convergence rate is 1/$\epsilon^2$}
        \item Define: $L(\wv) = \min_b \sum_{(\xv,y)\in S} [1-y(\langle \wv, \xv\rangle - b)]_+$
        \item Search $b$ in an outer loop

            {\color{green}Cons: evaluating objective is $1/\epsilon^2$}
    \end{itemize}
\end{frame}

\begin{frame}{Discussion}
    \begin{itemize}
        \item Pegasos: Simple $\&$ Efficient solver for SVM
        \item Sample vs. computational complexity
            \begin{itemize}
                \item Sample complexity: How many examples do we need as a function of VC-dim($\lambda$), accuracy($\epsilon$), and confidence($\delta$)
                \item in Pegasos, we aim at analyzing computational complexity based on $\lambda$, $\epsilon$, $\delta$ (also in Bottou $\&$ Bousquet)
            \end{itemize}
        \item Finding argmin vs. calculating min: It seems that Pegasos finds the argmin more easily than it requires to calculate the min value
    \end{itemize}
\end{frame}
