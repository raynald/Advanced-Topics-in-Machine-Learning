\begin{frame}{Previous Work}
    \begin{itemize}
        \item Dual-based methods
            \begin{itemize}
                \item Interior Point
                    \begin{itemize}
                        \item Memory: $n^2$, time: $n^3 \log(\log(1/\epsilon))$, run time per iteration $n^3$
                    \end{itemize}
                \item Decomposition
                    \begin{itemize}
                        \item Memory: $n$, time complexity: super-linear in $n$, slow convergence
                    \end{itemize}
            \end{itemize}
        \item Online learning $\&$ Stochastic Gradient
            \begin{itemize}
                \item Memory: $O(1)$, time: $1/\epsilon^2$ (linear kernel), run-time per iteration: O(d)
            \end{itemize}
    \end{itemize}
    \fcolorbox{yellow}{yellow}{Better rates for finite dimensional instances (Murata, Bottou)} 
    Typically, online learning algorithms do not converge to the optimal solution of SVM
\end{frame}

\begin{frame}{Basic Pegasos Algorithm (SGD)}
    {\color{blue} Algorithm}

    \begin{enumerate} 
        \item Choose $\wv_1 = 0 \in \R^d$
        \item Iterate for $t = 1,2,$\ldots$,T$
        \begin{enumerate}
            \item Choose {\color{red} $A_t$} $\subset S = \{1,2,\ldots,n\},{\color{red}|A_t|=k}$, uniformly at random
            \item Set stepsize $\eta_t \leftarrow \frac{1}{\lambda t}$
            \item Update $w^{(t+1)} \leftarrow w^{(t)} - \eta_t \partial f_{{\color{red}A_t}}(\wv^{ (t)})$
        \end{enumerate}
    \end{enumerate}

    {\color{blue} Theorem}

    For $\overline{\wv} = \frac1T \sum_{t=1}^T \wv_t $, we have:
    \[
        \E[f(\overline{\wv})]\le f(w^*) + {\color{red} c} \cdot \frac{1+\ln(T)}{2\lambda T}
    \]
    where $c=4R^2$.
\end{frame}

\begin{frame}{Basic Pegasos Algorithm (SGD)}
    {\color{blue} Algorithm}

    \begin{enumerate} 
        \item Choose $\wv_1 = 0 \in \R^d$
        \item Iterate for $t = 1,2,$\ldots$,T$
        \begin{enumerate}
            \item Choose {\color{red} $A_t$} $\subset S = \{1,2,\ldots,n\},{\color{red}|A_t|=k}$, uniformly at random
            \item Set stepsize $\eta_t \leftarrow \frac{1}{\lambda t}$
            \item Update $w^{(t+1)} \leftarrow (1-\eta_t\lambda)w^{(t)} + \frac{\eta_t}{\color{red}k} \sum_{i\in {\color{red} A_t}} l' \xv_i$
        \end{enumerate}
    \end{enumerate}

    {\color{blue} Theorem}

    For $\overline{\wv} = \frac1T \sum_{t=1}^T \wv_t $, we have:
    \[
         \E[f(\overline{\wv})]\le f(w^*) + {\color{red} c} \cdot \frac{1+\ln(T)}{2\lambda T}
    \]
    where $c=4R^2$.
\end{frame}

\begin{frame}{Run-Time of Pegasos}
    \begin{itemize}
        \item Choosing $|A_t|=1$% and a linear kernel over $\R^n$
            \begin{itemize}
                \item[$\rightarrow$] Run-time required for Pegasos to find $\epsilon$ accurate solution %w.p. , $1-\delta$
            \[
                \tilde{O}(\frac{d}{\lambda \epsilon})
            \]
            %\[
            %    \tilde{O}(\frac{n}{\delta \lambda \epsilon})
            %\]
            \end{itemize}
        \item Run-time does not depends on \#examples, suited for learning form large datasets
        \item Depends on ``difficulty'' of problem (both $\lambda$ and $\epsilon$)
    \end{itemize}
\end{frame}

%\begin{frame}{Formal Properties}
%    \begin{itemize}
%        \item Definition: $\wv$ is $\epsilon$ accurate if $f(\wv)-f(\wv^*)\le \epsilon$
        %\item Theorem 1: Pegasos finds $\epsilon$ accurate solution w.p., $1-\delta$ after at most
        %    \[
        %        \tilde{O}(\frac{1}{\delta \lambda \epsilon})
        %    \]
        %    iterations.
        %\item Theorem 2: Pegasos finds $\log(1/\delta)$ solutions $\st$ w.p., at least one of them is $\epsilon$ accurate after 
        %    \[
        %        \tilde{O}(\frac{\log(1/\delta)}{\lambda \epsilon})
        %    \]
        %    iterations.
%    \end{itemize}
%\end{frame}

\section{Analysis - faster convergence rates}
\begin{frame}{How to achieve this?}
$\|\wv^{(t+1)}-\wv^*\|^2 = \|\wv^{(t)} - \eta_t\chiv_i^{(t)} - \wv^*\|^2$
$= \|\wv^{(t)} - \wv^*\|^2 + \eta_t^2\|\chiv_i^{(t)}\|^2 - 2\eta_t\chiv_i^{(t)}(\wv^{(t)} - \wv^*    )$


Taking the expectation on both sides over the recent step
\[
    \E_{i^{(t)}}[ \|\wv^{(t+1)}-\wv^*\|^2  \ |\  \wv^t ] 
\]
\[
        = \|\wv^{(t)} - \wv^*\|^2 + \eta_t^2\E_{i^{(t)}}\|\chiv_i^{(t)}\|^2 
    - 2\eta_t\chiv_i^{(t)}(\wv^{(t)} - \wv^*)
\]
\[
    \le \|\wv^{(t)} - \wv^*\|^2 + \eta_t^2\E_{i^{(t)}}\|\chiv_i^{(t)}\|^2 - 2\eta_t[f(\wv^{(t)}) -      f(\wv^*) + \frac{\lambda}{2}\|\wv^{(t)}-\wv^*\|^2].
\]
Re-arranging and taking expectation over the whole process again
\[
    \E f(\wv^{(t)})-f(\wv^*) \le \frac{\eta_t}{2}\E_{i^{(t)}}\|\chiv_i^{(t)}\|^2+\frac{1-\lambda     \eta_t}{2\eta_t} \E \|\wv^{(t)}-\wv^*\|^2 
\]
\[
    - \frac{1}{2\eta_t}\E\left[  \|\wv^{(t+1)} - \wv^*\|^2  \ |\        \wv^t  \right] 
\]
\end{frame}

\begin{frame}{Lemma}
With the Lipschitz assumption on $l(y,u)$ that $\|l'(y,u)\| \le \ML$, and assuming that $\|\xv_i\|\le     R$ $\forall i$ where $i$ is picked according to $p_i$, it holds that
\[
    \E_{i^{(t)}} \|\chiv_i^{(t)}\|\le 4\ML^2R^2
\]
where $l'(y,u)$ denotes any subgradient with respect to the second variable.

\end{frame}

\begin{frame}{How to prove it?}
    \begin{block}{ Minkowski inequality}
    \[
        \sqrt{\E(X+Y)^2} \le \sqrt{\E X^2} + \sqrt{\E Y^2}
    \]
    \end{block}
    \[
        \chiv_i^{(t)}(\wv^{(t)}) = l'\xv_i + \lambda \wv^{(t)}, \wv^{(t+1)} = \wv^{(t)}-\eta_t\chiv_i
    ^{(t)}    
    \]
    \[
        \sqrt{\E_{i^{(t)}} \|\chiv_i^{(t)}\|^2} \le \sqrt{\E_{i^{(t)}} \| l'\xv_i \|^2} + \lambda           \sqrt{\E_{i^{(1:t-1)}} \| \wv^{(t)} \|^2} %\le \ML R + 
    \] 
    %\[
    %    \lambda \sqrt{\E_{i^{(1:t-1)}} \| \wv^{(t)}          \|^2}.
    %\]
    \[
        \sqrt{\E_{i^{(1:t)}} \|\wv^{(t+1)}\|^2} \le (1-\lambda \eta_t) \sqrt{\E_{i^{(1:t-1)}} \|\wv^{(t)}\|^2} +        \eta_t \sqrt{\E_{i(t)} \|l'\xv_i\|^2} 
    \]
    %\[
    %    \le (1-\lambda \eta_t)\sqrt{\E_{i(t)} \| \wv^{(t)}\|^2} +         \lambda \eta_t \frac{\ML R}{\lambda}.
    %\]
    {\color{blue} Why we don't need projection?}
\end{frame}

\begin{frame}{Analysis from Lacoste-Julien et.al.[2]}
    \begin{itemize} 
        \item Classical analysis: $\eta_t=\frac{1}{\lambda t}$
            \begin{itemize}
                \item $\E f\!\left(\frac1T\sum_{t=1}^T \wv^{(t)} \right)- f(\wv^*) \le \frac{2{\ML}^2 R^2}{\lambda T}(\ln{T}+1)$
                \item For Hinge loss $\ML = 1$, the result is same as before.
            \end{itemize}
        \item New analysis: $\eta_t=\frac{2}{\lambda (t+1)}$
            \begin{itemize}
                \item $\E f(\frac{2}{T(T+1)}\sum_{t=1}^{T}t w^{(t)}) - f(w^*) \le \frac{8{\ML}^2 R^2}{\lambda (T+1)}$
                \item $\E_{i(T)} \left[  \|\wv^{(T+1)} - \wv^*\|^2  \|  \wv^t  \right]  \le \frac{16{\ML}^2 R^2}{\lambda^2 (T+1)}$
                \item In this case, $\overline{w}^{(T)}\doteq \frac{2}{T(T+1)}\sum_{t=1}^T t\wv^{(t)}$
            \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}{Stochastic Dual Coordinate Ascent}
    \begin{itemize}
        \item Dual problem
            \[
                \max_{\alphav\in \R^n, 0\le\alphav_i\le 1} D(\alphav):=\frac1n \sum_{i=1}^n \alphav_i-\frac{1}{2\lambda n^2}\sum_{i=1}^n \alphav^T\Qv\alphav.
            \]
            where 
            \[
                 \Qv\in \R^{n\times n}, \Qv_{i,j}=y_iy_j\langle \xv_i, \xv_j\rangle,
            \]
        \item Relationship with primal variable: 
            \[ 
                 \wv(\alphav) := \frac{1}{\lambda n} \sum_{i=1}^n \alphav_i y_i \xv_i.
            \]
        \item Traditional SDCA
            \[
                 \alphav_i^{(t+1)} = \alphav_i^{(t)} + \deltav_i^{(t)}e_i,
            \]
    \end{itemize}
\end{frame}
